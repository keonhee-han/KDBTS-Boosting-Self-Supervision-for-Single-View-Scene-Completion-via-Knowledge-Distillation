defaults:       ## Note: Make sure the configuration is the same with the one from the training set up as it evaluates the occupancy from the checkpoint trained before.
    - default
    - data: kitti_360_DFT
    - _self_

# name: "eval_lidar_occ"
name: "Tst_KDDFT_eval_lidar_occ"
model: "bts_lidar"
#checkpoint: "out/kitti_360/enc0_chkpt_unfr_lr1e-6_BTS_DFT_nry_backend-None-1_20230824-232011/"  ## change whenever you want to evaluate the model
# checkpoint: "out/kitti_360/att32_nmv5_lay3_enc0_Unfr_Lr5e_5_BTS_Nry_backend-None-1_20230830-022311"  ## change whenever you want to evaluate the model
# checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/kitti_360/BaseExperiment_debug_backend-None-1_20230919-173035"  ## change whenever you want to evaluate the model


## Please change the corresponding checkpoint to evaluate the model
## No checkpt. # checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/kitti_360/__Ablation_encLyer1_slurm_backend-None-1_20231018-130507"  ## change whenever you want to evaluate the model
# checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/kitti_360/__Ablation_doZero_slurm_backend-None-1_20231018-130606"  ## change whenever you want to evaluate the model
# checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/kitti_360/__Ablation_do05_slurm_backend-None-1_20231018-170052"  ## change whenever you want to evaluate the model
# checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/kitti_360/__Ablation_embTypeff_slurm_backend-None-1_20231018-170112"  ## change whenever you want to evaluate the model
## No checkpt. # checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/kitti_360/__Ablation_embdim16_slurm_backend-None-1_20231018-170134"  ## change whenever you want to evaluate the model


## Knowledge Distillation KDBTS
# checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/pretrained/KD_FT_from_longrun_B4_lr3e-5_backend-None-1_20231005-123142"  ## change whenever you want to evaluate the model
# checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/pretrained/knowledge_distillation_longrun_finetuning_from_longrun_B4_backend-None-1_20231005-122402"  ## change whenever you want to evaluate the model
checkpoint: "/storage/user/muhled/outputs/mvbts/kitti-360/BaseExperimentNoFisheyeFeatureFusionLargeKnowDist_backend-None-1_20231112-111722/training_checkpoint_190000.pt"

## pretrained KD-DFT
# checkpoint: "/storage/user/hank/BehindTheScenes/out/mvbts/pretrained/KDDFT/training_checkpoint_400000.pt"
# checkpoint: "/storage/user/hank/BehindTheScenes/out/mvbts/kitti_360/__KD_tempOffsetFeStereo_backend-None-1_20231112-203616/training_checkpoint_320000.pt"

## fine-tuned MVBTS
# checkpoint: "/usr/stud/hank/storage/user/BehindTheScenes/out/mvbts/pretrained/long_training"
# checkpoint: "/storage/user/muhled/outputs/mvbts/kitti_360/long_training_random_offset/training_checkpoint_303500.pt"

# /usr/stud/hank/storage/user/BehindTheScenes/out/kitti_360/BTS_DFT_nry_frozen_enc01_backend-None-1_20230822-182806/training_checkpoint_174000.pt
# /usr/stud/hank/storage/user/BehindTheScenes/out/kitti_360/att32_nmv5_lay3_enc0_Unfr_Lr5e_5_BTS_Nry_backend-None-1_20230830-022311/training_checkpoint_324500.pt
output_path: "out/mvbts/kitti_360/"

log_every: 10
batch_size: 4
num_workers: 1


data:
    image_size: [192, 640]
    data_3d_bboxes: false
    data_segmentation: false
    data_fisheye: true          ## default: false
    data_stereo: true           ## default: false
    data_fc: 2                  ## default: 1. 2 => gives 8 views with 2 time stamps
    is_preprocessed: false

renderer:
    n_coarse : 64                       # default: 64, num sampling on a ray
    n_fine : 0
    n_fine_depth : 0
    depth_std : 1.0
    sched : []
    white_bkgd : false
    lindisp: true
    hard_alpha_cap: true
    eval_batch_size: 32768 # ==2^15       ## ~/2 /2 /2    ## Note: this is the absolute eval steps without having to worry about the variant evaluations caused by batch epoch

model_conf:
    arch: "MVBTSNet"            # !Note!: this should be the same architecture with the architecture trained with checkpt to eval. constructor for class MVBTSNet(torch.nn.Module):
    use_code: true
    prediction_mode: default
    num_multiviews: 3           # TODO: Check L_{loss} and L_{render} which neither of them works. number of multi views to aggregate. Possible value ~8. Default: 2 Note: cams are render loss
    sample_color: true
    use_viewdirs: false                  # use with Neuray to aggregate multi-views, set true. c.f. IBRNet, PixelNeRF
    # loss_from_single_img: false # default: false
    encoder:
        type: "monodepth2"
        freeze: false           # freezing encoder to train the transformer
        pretrained: true
        resnet_layers: 50
        num_ch_dec: [32,32,64,128,256]
        d_out: 64               # c.f. paper D.1. output channel dimension 64 as hparam for the best result

    code:
        num_freqs: 6
        freq_factor: 1.5
        include_input: true

    decoder_heads:
        - type: "MultiViewHead"
          name: "multiviewhead"
          args:
            embedding_encoder:
                type: "pwf"
                d_out: 32
            independent_token:
                type: "FixedViewIndependentToken"
                args: None
            attn_layers:
                IBRAttn: False
                n_layers: 2
                n_heads: 4
                readout_token:
                    type: "fixed"  # fixed, data, neuray
            probing_layer:

            dropout_views_rate: 0.0      # set 0 for disable and for knowledge-distillation
            dropout_multiviewhead: false # enforce encoder of multiviews to be dropped out, and keep the 1st encoder index not to be affected, so that to simulate different camera configurations with a varying number of input images during training. However, we want at least the first image to be consistently be present in the training. So no dropout for the first image. Dropout does not affect the single view head. 

        - type: "resnet"
          name: "singleviewhead"
          args:
            n_blocks : 0
            view_number: 0
            d_hidden : 64
            
    final_prediction_head: "multiviewhead"  # default: multiviewhead, this decides whether the final prediction head gives single or multiple prediction results. Note: This comes with pseudo GT loss


    encoding_style: "random"                # random | default | stereo : execlusively load stereo for input features map. Note: Current kitti-360 fisheye's intrinsic is poorly setup, better to turn it false to stop detrimental result for density field reconstruction. random: randomly sampling feature maps during training fisheye during training. BTS uses only mono as sampling. default: encoding indices include num multiview in order.
    
    # viz purpose: set empty for non-saving bin files
    save_bin_path: 
    #"/storage/user/hank/methods_test/semantic-kitti-api/bts_test/sequences/00/voxels"                       
    
    ## camera will evaluate the possible cases up until the defined indices.
    # encoder_ids: [0]             # which cam to choose for inference of viz_eval. e.g. [0, 1 ...] or set "false" if you want to sample randomly (c.f. fisheye_offset)
    # encoder_ids: [0, 1]                     ## temporal mono case
    # encoder_ids: [0, 2]                     ## stereo fixed case
    encoder_ids: [0, 1, 2, 3]               ## stereo temporal case
    # encoder_ids: [0, 2, 4, 6]                 ## fisheye fixed case
    # encoder_ids: [0, 1, 2, 3, 4, 5, 6, 7]   ## full case
    # encoder_ids: full       ## mono | stereo | temporal

    # encoder_ids: false     # which cam to choose for inference of viz_eval. e.g. [0, 1 ...] or set "false" if you want to sample randomly (c.f. fisheye_offset)

    z_near: 3
    z_far: 80                           # default: 80m
    inv_z: true

    n_frames_encoder: 1
    n_frames_render: 2                  # num frame to render among v==8
    frame_sample_mode: kitti360-mono

    sample_mode: patch                  # e.g. a ray_batch_size of 4096, we sample 4096/64=64 patches.
    patch_size: 8                       # => 64 patches * (8 * 8) pixels / patch * 1 ray / pixel * 64 points / ray = 262144 points in the space in a single batch entry to be evaluated
    ray_batch_size: 2048                # default: 2048, 2048 in BTS paper #_sampled points in an array from one single view. Note: assert ray_batch_size % (self.patch_size_x * self.patch_size_y)
    flip_augmentation: true             # data augmentation

    learn_empty: false
    code_mode: z

scheduler:
    type: step
    step_size: 120000
    gamma: 0.1