defaults:                       ### This code is for concept of proof and transfer its parameters to long run slurm training config
    - default
    - data: kitti_360_DFT
    - _self_

#### Simply comment one of them for usage. (Because we want the same configuration as the model was trained last time.)

## For eval_lidar_occ.yaml
# name: "eval_lidar_occ"
# model: "bts_lidar"
# # checkpoint: "out/kitti_360/BTS_DFT_nry_frozen_enc01_backend-None-1_20230822-182806/"  ## change whenever you want to evaluate the model
# checkpoint: "out/kitti_360/BTS_DFT_nry_frozen_enc0_backend-None-1_20230822-232314"  ## change whenever you want to evaluate the model
# log_every: 10
# batch_size: 4
# num_workers: 1

# data:
#     image_size: [192, 640]
#     data_3d_bboxes: false
#     data_segmentation: false
#     data_fisheye: true          ## default: false
#     data_stereo: true           ## default: false
#     data_fc: 2                  ## default: 1. 2 => gives 8 views with 2 time stamps
#     is_preprocessed: false

## For training
name: "BaseExperiment_debug"     ## Please name it in case there are various hparam settings to distinguish the diff results easily.
model: "bts"                    # model: "bts"/ "bts_overfit"    # TODO: bts_~~ for debug ## need to modify for debugging for different bts model ## replace en-decoder for trained one
# output_path: "/storage/user/muhled/outputs/mvbts/kitti-360"
output_path: "out/debug/mvbts/kitti-360"


num_epochs: 200
validate_every: 1
visualize_every: 1             # default: 10       ## bigger is good for computational reduction
batch_size: 1                  # default: 16       ## Note: size 1 is working properly with NueRay integration. The complete integration needs to be done with super-batch size in training loop. ## each batch contains 8 views 16*8 too much

save_best:
    metric: abs_rel             # The model that has the smallest absolute relative error will be saved.
    sign: -1                    # The sign is negative, meaning that the model with the smallest abs_rel is the best

# resume_from: "/storage/user/hank/BehindTheScenes/out/kitti_360/pretrained/training-checkpoint.pt"
# resume_from: "/storage/user/muhled/outputs/mvbts/kitti_360/long_training/training_checkpoint_310000.pt"
new_model_architecture: True

data:
    data_fc: 2              ## default: 2, 3 for monocular camera
    image_size: [192, 640]
    color_aug: true
    is_preprocessed: true
    fisheye_rotation: [0, -15]

    ## This is for ablation study for different training view cases: F, S, FS in BTS paper
    data_fisheye: true
    data_stereo: true       ## load only left

#### below are the same for both eval and training

renderer:
    n_coarse : 64                       # default: 64, num sampling on a ray
    n_fine : 0
    n_fine_depth : 0
    depth_std : 1.0
    sched : []
    white_bkgd : false
    lindisp: true
    hard_alpha_cap: true
    eval_batch_size: 32768 # ==2^15       ## ~/2 /2 /2    ## Note: this is the absolute eval steps without having to worry about the variant evaluations caused by batch epoch

model_conf:
    arch: "MVBTSNet"            # constructor for class MVBTSNet(torch.nn.Module):
    use_code: true
    prediction_mode: default
    num_multiviews: 1           # TODO: Check L_{loss} and L_{render} which neither of them works. number of multi views to aggregate. Possible value ~8. Default: 2 Note: cams are render loss
    use_viewdirs: true          # to aggregate multi-views, set true. c.f. IBRNet
    
    # camera_positional_encoding: true

    encoder:
        type: "monodepth2"
        freeze: false            # freezing encoder to train the en- and decoder
        pretrained: true
        resnet_layers: 50
        num_ch_dec: [32,32,64,128,256]
        d_out: 64               # c.f. paper D.1. output channel dimension 64 as hparam for the best result

    code:
        num_freqs: 6
        freq_factor: 1.5
        include_input: true

    decoder_heads:
        - type: "MultiViewHead"
          name: "multiviewhead"
          freeze: false                 # For knowledge-distillation learning for singleview head
          args:
            embedding_encoder:
                type: "pwf"          ## set embedding type as string: pwf | ff | hpwf : PoswiseFF_emb4enc | FeedForward | FeedForward_half| hardcoded_PoswiseFF_emb4enc . Note: shrink the size of sequence of dimension of token (for model's complexity reason)
                d_out: 4           ## input dimension of encoder layer of Transformer
            independent_token:
                type: "FixedViewIndependentToken"
                args: None
            # independent_token:
            #     type: "NeuRayIndependentToken"
            #     args:
            #         n_points_per_ray: 64 # same as renderer.n_coarse
            attn_layers:
                IBRAttn: False      ## default: False
                n_layers: 1         ## default: 3, num encoder layers from Transformer
                n_heads: 4
                readout_token:
                    type: "fixed"  # fixed, data, neuray
            probing_layer:

            dropout_views_rate: 0.5       # set 0 for disable and for knowledge-distillation
            dropout_multiviewhead: true # enforce encoder of multiviews to be dropped out, and keep the 1st encoder index not to be affected, so that to simulate different camera configurations with a varying number of input images during training. However, we want at least the first image to be consistently be present in the training. So no dropout for the first image. Dropout does not affect the single view head. 

        - type: "resnet"
          name: "singleviewhead"
          args:
            n_blocks : 0
            view_number: 0      # which encoder id we want to extract among feature maps
            d_hidden : 64
    final_prediction_head: "multiviewhead"

    encoding_style: "stereo"                # random | default | stereo : execlusively load stereo for input features map. Note: Current kitti-360 fisheye's intrinsic is poorly setup, better to turn it false to stop detrimental result for density field reconstruction. random: randomly sampling feature maps during training fisheye during training. BTS uses only mono as sampling. default: encoding indices include num multiview in order.
    ids_enc_offset_viz: [0]     # which cam to choose for inference of viz_eval. e.g. [0, 1 ...] or set "false" if you want to sample randomly (c.f. fisheye_offset)

    z_near: 3
    z_far: 80                           # default: 80m
    inv_z: true

    n_frames_encoder: 1
    n_frames_render: 2                  # num frame to render among given cameras at a frame: 4 x 2 => v==8
    frame_sample_mode: mono             # defaut: kitti360-mono | mono | stereo | only (for single image without temporal steps)

    sample_mode: patch                  # e.g. a ray_batch_size of 4096, we sample 4096/64=64 patches.
    patch_size: 8                       # => 64 patches * (8 * 8) pixels / patch * 1 ray / pixel * 64 points / ray = 262144 points in the space in a single batch entry to be evaluated
    ray_batch_size: 64                 # default: 2048, 2048 in BTS paper #_sampled points in an array from one single view. Note: assert ray_batch_size % (self.patch_size_x * self.patch_size_y)
    flip_augmentation: true             # data augmentation

    learn_empty: false
    code_mode: z                        # default: z, z | distance | z_feat: additional cam_feature_map_positional_encoding, Note: z_feat is unverified

loss:
    criterion: "l1+ssim"                # photometric loss
    invalid_policy: weight_guided
    lambda_edge_aware_smoothness: 1e-3     # default: 0.001
    lambda_pseudo_ground_truth: 0        # default: 1e-4, set 0 to disable
    pseudo_ground_truth_teacher: "multiviewhead"
    pseudo_ground_truth_students:
        - "singleviewhead"          ## making list type with an string element contained

learning_rate: 1e-4                     # default: 1e-4

scheduler:
    type: step
    step_size: 120000
    gamma: 0.1

#backend = "nccl"  # Or "gloo"
#nproc_per_node: 2

IDE_debug:                              ## set true for remote IDE debug for pydevd_pycharm
    sv_: 58
    port_: 58023

#This YAML configuration file specifies settings for training a BTSNet neural network model on the KITTI 360 dataset. It outlines parameters for various aspects of the training, including data processing, model configuration, loss functions, and the training schedule. Here are some key details:
#General Settings: The model to be used is BTSNet. The output from this training run will be stored in the directory "out/kitti_360". The training will run for 25 epochs with a batch size of 16.
#Data Parameters: The data configuration specifies that the model will use color augmentation and preprocessing. The images will be resized to a size of 192x640. The fisheye_rotation parameter is set to [0, -15] degrees, suggesting some form of geometric transformation on the input data.
#Model Configuration: The main architecture of the model is BTSNet. The encoder type is "monodepth2", a popular model for depth estimation, with 50 ResNet layers. The mlp_coarse and mlp_fine parameters dictate the architecture of the multi-layer perceptron (MLP) used in the coarse and fine stages of BTSNet respectively. The MLP for the coarse stage is a resnet with 0 blocks and 64 hidden dimensions, while the MLP for the fine stage is an "empty" type with 1 block and 128 hidden dimensions.
#Loss Parameters: The loss function used is a combination of L1 and SSIM (Structural Similarity Index Measure) losses. The policy for handling invalid values in the loss computation is "weight_guided". The lambda_edge_aware_smoothness is a weight parameter for the edge-aware smoothness loss term, which encourages the model to produce depth maps with smooth transitions except at the edges of objects.
#Scheduler Parameters: The scheduler type is "step", indicating that the learning rate will be decreased at certain intervals (step size of 120000) by a factor of 0.1.
#Renderer Parameters: The renderer will use 64 coarse samples, and no fine samples. Other parameters include the standard deviation for the depth (depth_std), a flag indicating whether to use linearized disparity (lindisp), and a flag for a hard alpha cap.
#These settings provide an overall blueprint for training the BTSNet model on the specified dataset.